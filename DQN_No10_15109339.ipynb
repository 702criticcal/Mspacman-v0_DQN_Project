{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "# matplotlib 설정\n",
    "from IPython.display import HTML\n",
    "import matplotlib\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# 일관된 출력을 위해 유사난수 초기화\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"MsPacman-v0\")\n",
    "obs = env.reset()\n",
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(9)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mspacman_color = 210 + 164 + 74\n",
    "\n",
    "def preprocess_observation(obs):\n",
    "    img = obs[1:176:2, ::2] # 자르고 크기를 줄입니다.\n",
    "    img = img.sum(axis=2) # 흑백 스케일로 변환합니다.\n",
    "    img[img==mspacman_color] = 0 # 대비를 높입니다.\n",
    "    img = (img // 3 - 128).astype(np.int8) # -128~127 사이로 정규화합니다.\n",
    "    return img.reshape(88, 80, 1)\n",
    "\n",
    "img = preprocess_observation(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9f6d7d4f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Original image (160×210 RGB)\")\n",
    "plt.imshow(obs)\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(122)\n",
    "plt.title(\"Preprocessed image (88×80 grayscale)\")\n",
    "plt.imshow(img.reshape(88, 80), interpolation=\"nearest\", cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"preprocessing_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "input_height = 88\n",
    "input_width = 80\n",
    "input_channels = 1\n",
    "conv_n_maps = [32, 64, 64]\n",
    "conv_kernel_sizes = [(8,8), (4,4), (3,3)]\n",
    "conv_strides = [4, 2, 1]\n",
    "conv_paddings = [\"SAME\"] * 3 \n",
    "conv_activation = [tf.nn.relu] * 3\n",
    "n_hidden_in = 64 * 11 * 10  # conv3은 11x10 크기의 64개의 맵을 가집니다\n",
    "n_hidden = 512\n",
    "hidden_activation = tf.nn.relu\n",
    "n_outputs = env.action_space.n  # 9개의 행동이 가능합니다\n",
    "initializer = tf.variance_scaling_initializer()\n",
    "\n",
    "def q_network(X_state, name):\n",
    "    prev_layer = X_state / 128.0 # 픽셀 강도를 [-1.0, 1.0] 범위로 스케일 변경합니다.\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        for n_maps, kernel_size, strides, padding, activation in zip(\n",
    "                conv_n_maps, conv_kernel_sizes, conv_strides,\n",
    "                conv_paddings, conv_activation):\n",
    "            prev_layer = tf.layers.conv2d(\n",
    "                prev_layer, filters=n_maps, kernel_size=kernel_size,\n",
    "                strides=strides, padding=padding, activation=activation,\n",
    "                kernel_initializer=initializer)\n",
    "        last_conv_layer_flat = tf.reshape(prev_layer, shape=[-1, n_hidden_in])\n",
    "        hidden = tf.layers.dense(last_conv_layer_flat, n_hidden,\n",
    "                                 activation=hidden_activation,\n",
    "                                 kernel_initializer=initializer)\n",
    "        outputs = tf.layers.dense(hidden, n_outputs,\n",
    "                                  kernel_initializer=initializer)\n",
    "    trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                       scope=scope.name)\n",
    "    trainable_vars_by_name = {var.name[len(scope.name):]: var\n",
    "                              for var in trainable_vars}\n",
    "    return outputs, trainable_vars_by_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_state = tf.placeholder(tf.float32, shape=[None, input_height, input_width,\n",
    "                                            input_channels])\n",
    "online_q_values, online_vars = q_network(X_state, name=\"q_networks/online\")\n",
    "target_q_values, target_vars = q_network(X_state, name=\"q_networks/target\")\n",
    "\n",
    "copy_ops = [target_var.assign(online_vars[var_name])\n",
    "            for var_name, target_var in target_vars.items()]\n",
    "copy_online_to_target = tf.group(*copy_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/conv2d/bias:0': <tf.Variable 'q_networks/online/conv2d/bias:0' shape=(32,) dtype=float32_ref>,\n",
       " '/conv2d/kernel:0': <tf.Variable 'q_networks/online/conv2d/kernel:0' shape=(8, 8, 1, 32) dtype=float32_ref>,\n",
       " '/conv2d_1/bias:0': <tf.Variable 'q_networks/online/conv2d_1/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " '/conv2d_1/kernel:0': <tf.Variable 'q_networks/online/conv2d_1/kernel:0' shape=(4, 4, 32, 64) dtype=float32_ref>,\n",
       " '/conv2d_2/bias:0': <tf.Variable 'q_networks/online/conv2d_2/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " '/conv2d_2/kernel:0': <tf.Variable 'q_networks/online/conv2d_2/kernel:0' shape=(3, 3, 64, 64) dtype=float32_ref>,\n",
       " '/dense/bias:0': <tf.Variable 'q_networks/online/dense/bias:0' shape=(512,) dtype=float32_ref>,\n",
       " '/dense/kernel:0': <tf.Variable 'q_networks/online/dense/kernel:0' shape=(7040, 512) dtype=float32_ref>,\n",
       " '/dense_1/bias:0': <tf.Variable 'q_networks/online/dense_1/bias:0' shape=(9,) dtype=float32_ref>,\n",
       " '/dense_1/kernel:0': <tf.Variable 'q_networks/online/dense_1/kernel:0' shape=(512, 9) dtype=float32_ref>}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "online_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "momentum = 0.95\n",
    "\n",
    "with tf.variable_scope(\"train\"):\n",
    "    X_action = tf.placeholder(tf.int32, shape=[None])\n",
    "    y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "    q_value = tf.reduce_sum(online_q_values * tf.one_hot(X_action, n_outputs),\n",
    "                            axis=1, keep_dims=True)\n",
    "    error = tf.abs(y - q_value)\n",
    "    clipped_error = tf.clip_by_value(error, 0.0, 1.0)\n",
    "    linear_error = 2 * (error - clipped_error)\n",
    "    loss = tf.reduce_mean(tf.square(clipped_error) + linear_error)\n",
    "\n",
    "    global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum,\n",
    "                                           use_nesterov=True)\n",
    "    training_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, maxlen):\n",
    "        self.maxlen = maxlen\n",
    "        self.buf = np.empty(shape=maxlen, dtype=np.object)\n",
    "        self.index = 0\n",
    "        self.length = 0\n",
    "        \n",
    "    def append(self, data):\n",
    "        self.buf[self.index] = data\n",
    "        self.length = min(self.length + 1, self.maxlen)\n",
    "        self.index = (self.index + 1) % self.maxlen\n",
    "    \n",
    "    def sample(self, batch_size, with_replacement=True):\n",
    "        if with_replacement:\n",
    "            indices = np.random.randint(self.length, size=batch_size) # 더 빠름\n",
    "        else:\n",
    "            indices = np.random.permutation(self.length)[:batch_size]\n",
    "        return self.buf[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_memory_size = 500000\n",
    "replay_memory = ReplayMemory(replay_memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_memories(batch_size):   \n",
    "    cols = [[], [], [], [], []] # 상태, 행동, 보상, 다음 상태, 계속\n",
    "    for memory in replay_memory.sample(batch_size):\n",
    "        for col, value in zip(cols, memory):\n",
    "            col.append(value)\n",
    "    cols = [np.array(col) for col in cols]\n",
    "    return cols[0],cols[1],cols[2].reshape(-1, 1),cols[3],cols[4].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_min = 0.1\n",
    "eps_max = 1.0\n",
    "eps_decay_steps = 2000000\n",
    "\n",
    "def epsilon_greedy(q_values, step):\n",
    "    epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps)\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_outputs) # exploration\n",
    "    else:\n",
    "        return np.argmax(q_values) # exploitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 4000000  # 전체 훈련 스텝 횟수\n",
    "training_start = 10000  # 10,000번 게임을 반복한 후에 훈련 시작\n",
    "training_interval = 4  # 4번 게임을 반복하고 훈련 스텝 실행\n",
    "save_steps = 1000  # 1,000번 훈련 스텝마다 모델 저장\n",
    "copy_steps = 10000  # 10,000번 훈련 스텝마다 Actual DQN을 Target DQN으로 복사\n",
    "discount_factor = 0.99\n",
    "skip_start = 90  # 게임의 시작 부분은 스킵\n",
    "batch_size = 50\n",
    "iteration = 0  # 게임 반복횟수\n",
    "checkpoint_path = \"./my_dqn.ckpt\"\n",
    "done = True # 환경 리셋의 필요 여부 판단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_val = np.infty\n",
    "game_length = 0\n",
    "total_max_q = 0\n",
    "mean_max_q = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "반복 853\t훈련 스텝 18001/4000000 (0.5)%\t손실   inf\t평균 최대-Q 0.154167   "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-b6e905a1ad3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Actual DQN으로 게임을 플레이합니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/atari_py/ale_python_interface.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0male_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgame_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_path + \".index\"):\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        init.run()\n",
    "        copy_online_to_target.run()\n",
    "    while True:\n",
    "        step = global_step.eval()\n",
    "        if step >= n_steps:\n",
    "            break\n",
    "        iteration += 1\n",
    "        print(\"\\r반복 {}\\t훈련 스텝 {}/{} ({:.1f})%\\t손실 {:5f}\\t평균 최대-Q {:5f}   \".format(\n",
    "            iteration, step, n_steps, step * 100 / n_steps, loss_val, mean_max_q), end=\"\")\n",
    "        if done: # 게임이 종료되면 다시 시작합니다\n",
    "            obs = env.reset()\n",
    "            for skip in range(skip_start): # 게임 시작 부분은 스킵합니다\n",
    "                obs, reward, done, info = env.step(0)\n",
    "            state = preprocess_observation(obs)\n",
    "\n",
    "        # Actual DQN이 해야할 Action을 평가합니다\n",
    "        q_values = online_q_values.eval(feed_dict={X_state: [state]})\n",
    "        action = epsilon_greedy(q_values, step)\n",
    "\n",
    "        # Actual DQN으로 게임을 플레이합니다.\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        next_state = preprocess_observation(obs)\n",
    "\n",
    "        # Replay buffer에 기록합니다\n",
    "        replay_memory.append((state, action, reward, next_state, 1.0 - done))\n",
    "        state = next_state\n",
    "\n",
    "        # 트래킹을 위해 통계값을 계산합니다\n",
    "        total_max_q += q_values.max()\n",
    "        game_length += 1\n",
    "        if done:\n",
    "            mean_max_q = total_max_q / game_length\n",
    "            total_max_q = 0.0\n",
    "            game_length = 0\n",
    "\n",
    "        if iteration < training_start or iteration % training_interval != 0:\n",
    "            continue\n",
    "        \n",
    "        # Target DQN을 사용해서 메모리에서 샘플링하여 Target Q value를 구합니다\n",
    "        X_state_val, X_action_val, rewards, X_next_state_val, continues = (\n",
    "            sample_memories(batch_size))\n",
    "        next_q_values = target_q_values.eval(\n",
    "            feed_dict={X_state: X_next_state_val})\n",
    "        max_next_q_values = np.max(next_q_values, axis=1, keepdims=True)\n",
    "        y_val = rewards + continues * discount_factor * max_next_q_values\n",
    "\n",
    "        # Actual DQN을 학습시킵니다\n",
    "        _, loss_val = sess.run([training_op, loss], feed_dict={\n",
    "            X_state: X_state_val, X_action: X_action_val, y: y_val})\n",
    "\n",
    "        # Actual DQN을 Target DQN으로 정해둔 일정 간격마다 복사합니다\n",
    "        if step % copy_steps == 0:\n",
    "            copy_online_to_target.run()\n",
    "\n",
    "        # 정해둔 일정 간격으로 저장합니다\n",
    "        if step % save_steps == 0:\n",
    "            saver.save(sess, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n"
     ]
    }
   ],
   "source": [
    "frames = []\n",
    "n_max_steps = 10000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "\n",
    "    obs = env.reset()\n",
    "    for step in range(n_max_steps):\n",
    "        state = preprocess_observation(obs)\n",
    "\n",
    "        # Actual DQN이 해야할 Action을 평가합니다\n",
    "        q_values = online_q_values.eval(feed_dict={X_state: [state]})\n",
    "        action = np.argmax(q_values)\n",
    "\n",
    "        # Actual DQN이 게임을 플레이합니다\n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "        img = env.render(mode=\"rgb_array\")\n",
    "        frames.append(img)\n",
    "\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(): 0,\n",
       " (97,): 3,\n",
       " (97, 115): 8,\n",
       " (97, 119): 6,\n",
       " (100,): 2,\n",
       " (100, 115): 7,\n",
       " (100, 119): 5,\n",
       " (115,): 4,\n",
       " (119,): 1}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_keys_to_action()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_scene(num, frames, patch):\n",
    "    plt.close()\n",
    "    patch.set_data(frames[num])\n",
    "    return patch,\n",
    "\n",
    "def plot_animation(frames, figsize=(5,6), repeat=False, interval=40):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    return animation.FuncAnimation(fig, update_scene, fargs=(frames, patch), \n",
    "                                   frames=len(frames), repeat=repeat, interval=interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = plot_animation(frames, figsize=(5,6))\n",
    "HTML(video.to_html5_video())  # HTML5 동영상으로 만들어 줍니다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
